---
description: Rules for writing and running tests
globs: ["**/tests/**/*.py", "**/test_*.py", "**/*_test.py", "conftest.py"]
alwaysApply: false
---

# Testing Rules

## TDD Workflow (Required)
1. **RED**: Write a failing test first
2. **GREEN**: Write minimal code to pass
3. **REFACTOR**: Clean up while tests stay green

## Test Structure
```python
import pytest
from unittest.mock import AsyncMock, patch

class TestFeatureName:
    """Tests for feature_name functionality."""
    
    @pytest.fixture
    def setup_data(self):
        """Fixture for test data."""
        return {"key": "value"}
    
    def test_success_case(self, setup_data):
        """Test description - what behavior is being tested."""
        # Arrange
        input_data = setup_data
        
        # Act
        result = function_under_test(input_data)
        
        # Assert
        assert result == expected_value
    
    def test_edge_case(self):
        """Test edge case handling."""
        with pytest.raises(ValueError):
            function_under_test(invalid_input)
```

## Naming Conventions
- Test files: `test_<module>.py`
- Test classes: `TestFeatureName`
- Test methods: `test_<behavior>_<condition>_<expectation>`

## Async Testing
```python
import pytest

@pytest.mark.asyncio
async def test_async_operation():
    result = await async_function()
    assert result is not None
```

## Mocking
- Mock external services, not internal logic
- Use `pytest-mock` or `unittest.mock`
- Prefer dependency injection over patching

## Running Tests
```bash
uv run pytest                    # All tests
uv run pytest tests/unit/        # Unit tests only
uv run pytest -k "test_name"     # Specific test
uv run pytest --cov=src          # With coverage
```

## TDD Metrics (Track These)
- **Time-to-green**: Time from test creation to passing
- **Iteration count**: Plan → Code → Test cycles per task
- **Diff size**: Lines changed per iteration (target: <200)
- **Green-test %**: Percentage of iterations where all tests pass
